{"cells": [{"cell_type": "code", "execution_count": 7, "id": "422b7196-db4f-46ad-ac22-903892b023b8", "metadata": {}, "outputs": [], "source": "# https://github.com/tthustla/pyspark_sa_gcp/blob/master/pyspark_sa.py\n\nimport sys\nimport pyspark as ps\nimport warnings\nimport re\nfrom pyspark.sql import functions as f\nfrom pyspark.sql import types as t\nfrom pyspark.sql.types import StringType\nfrom pyspark.ml.feature import Tokenizer, NGram, CountVectorizer, IDF, StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import PipelineModel"}, {"cell_type": "code", "execution_count": 8, "id": "2d40fad6-a343-4cd6-8a7e-68e33757b9ae", "metadata": {}, "outputs": [], "source": "#define regex pattern for preprocessing\npat1 = r'@[A-Za-z0-9_]+'\npat2 = r'https?://[^ ]+'\ncombined_pat = r'|'.join((pat1,pat2))\nwww_pat = r'www.[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                \"mustn't\":\"must not\"}\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"}, {"cell_type": "code", "execution_count": 9, "id": "604ca6db-9f8c-4472-885e-ed58e8639caf", "metadata": {}, "outputs": [], "source": "# preprocessing for\n# first_process: to remove Twitter handle and URL\n# second_process: to remove URL pattern starting with www.\n# third_process: to lower characters\n# fourth_process: to replace contracted negation with proper forms\n# result: remove numbers and special characters\ndef pre_processing(column):\n    first_process = re.sub(combined_pat, '', column)\n    second_process = re.sub(www_pat, '', first_process)\n    third_process = second_process.lower()\n    fourth_process = neg_pattern.sub(lambda x: negations_dic[x.group()], third_process)\n    result = re.sub(r'[^A-Za-z ]','',fourth_process)\n    return result.strip()"}, {"cell_type": "code", "execution_count": 10, "id": "82e2c033-52c9-499f-af58-b38c864e973c", "metadata": {}, "outputs": [], "source": "# build a pipeline following below order\n# tokenizer: split a tweet into individual words\n# ngrams: create n-gram representation of words. Here it's set to trigram\n# cv: turn n-gram representaion to a sparse representaion of the token counts. Below 5,460 is used as vocabulary size\n# idf: calculate inverse document frequency from the result of the previous step \n#      to diminishes the weight of terms that occur very frequently in the document set \n#      and increases the weight of terms that occur rarely.\n# assembler: transform the result of previous step to a single feature vector\n# label_stringIdx: encode target labels to a column of label indices. \n#                  The indices are ordered by label frequencies, so the most frequent label gets index 0.\n# lr: fit logistic regression with 'features' and 'label'\ndef build_pipeline():\n    tokenizer = [Tokenizer(inputCol='tweet',outputCol='words')]\n    ngrams = [NGram(n=i, inputCol='words', outputCol='{0}_grams'.format(i)) for i in range(1,4)]\n    cv = [CountVectorizer(vocabSize=5460, inputCol='{0}_grams'.format(i), outputCol='{0}_tf'.format(i)) for i in range(1,4)]\n    idf = [IDF(inputCol='{0}_tf'.format(i), outputCol='{0}_tfidf'.format(i), minDocFreq=5) for i in range(1,4)]\n    assembler = [VectorAssembler(inputCols=['{0}_tfidf'.format(i) for i in range(1,4)], outputCol='features')]\n    label_stringIdx = [StringIndexer(inputCol='sentiment', outputCol='label')]\n    # accuracy with saved model on test data is 0.8099724688360349\n    class_model = [LogisticRegression(maxIter=100)]\n    pipeline = Pipeline(stages=tokenizer+ngrams+cv+idf+assembler+label_stringIdx+class_model)\n    return pipeline"}, {"cell_type": "code", "execution_count": 11, "id": "9c196f7a-a6f0-4603-a476-ba04c857813f", "metadata": {}, "outputs": [], "source": "# below main function can be use for either first training or getting predictions with a loaded model\n# first retrieve data\n# apply pre-processing by making the above defined pre_processing function to a user defined function\n# either build the pipeline from the above build_pipeline function and train or use a loaded pipeline model\n# make predictions on the test set\n# output the pipeline model, Spark dataframe of the predictions, and the prediction accuracy on the test set\n\ninput_train=\"gs://cloud-project-bucket-3/pyspark_sa_train_data.csv\"\ninput_test=\"gs://cloud-project-bucket-3/pyspark_sa_test_data.csv\"\n\ndef main(sqlc,input_dir,loaded_model=None):\n    print('retrieving data from {}'.format(input_dir))\n    if not loaded_model:\n        # train_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(input_dir+'training_data.csv')\n        train_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(input_train)\n    # test_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(input_dir+'test_data.csv')    \n    test_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(input_test)\n    print('preprocessing data...')\n    reg_replaceUdf = f.udf(pre_processing, t.StringType())\n    if not loaded_model:\n        train_set = train_set.withColumn('tweet', reg_replaceUdf(f.col('text')))\n    test_set = test_set.withColumn('tweet', reg_replaceUdf(f.col('text')))\n    if not loaded_model:\n        pipeline = build_pipeline()\n        print('training...')\n        model = pipeline.fit(train_set)\n    else:\n        model = loaded_model\n    print('making predictions on test data...')\n    predictions = model.transform(test_set)\n    accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_set.count())\n    return model, predictions, accuracy"}, {"cell_type": "code", "execution_count": 13, "id": "03f3843a-56d4-44f0-b8be-9232d1f1e29d", "metadata": {}, "outputs": [], "source": "sc.stop()"}, {"cell_type": "code", "execution_count": null, "id": "ae2e41cc-3683-4f94-b77d-22e6ffde4a7c", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}