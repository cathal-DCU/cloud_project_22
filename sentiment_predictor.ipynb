{"cells": [{"cell_type": "code", "execution_count": 115, "id": "9aa958e6-575b-48a4-8fc0-cba3d1606fa5", "metadata": {}, "outputs": [], "source": "# https://github.com/tthustla/pyspark_sa_gcp/blob/master/pyspark_sa.py\nimport pandas as pd\nimport random\nimport sys\nimport pyspark as ps\nimport warnings\nimport re\nfrom pyspark.sql import functions as f\nfrom pyspark.sql import types as t\nfrom pyspark.sql.types import StringType\nfrom pyspark.ml.feature import Tokenizer, NGram, CountVectorizer, IDF, StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import PipelineModel"}, {"cell_type": "code", "execution_count": 116, "id": "a878ace7-b4d3-4ce5-9b4f-9b21d53710ff", "metadata": {}, "outputs": [], "source": "#define regex pattern for preprocessing\npat1 = r'@[A-Za-z0-9_]+'\npat2 = r'https?://[^ ]+'\ncombined_pat = r'|'.join((pat1,pat2))\nwww_pat = r'www.[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                \"mustn't\":\"must not\"}\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\ninput_test=\"gs://cloud-project-bucket-3/batman_col_names.csv\""}, {"cell_type": "code", "execution_count": 117, "id": "d24c5876-cc89-4f42-896d-18709d3ebc89", "metadata": {}, "outputs": [], "source": "# preprocessing for\n# first_process: to remove Twitter handle and URL\n# second_process: to remove URL pattern starting with www.\n# third_process: to lower characters\n# fourth_process: to replace contracted negation with proper forms\n# result: remove numbers and special characters\ndef pre_processing(column):\n    first_process = re.sub(combined_pat, '', column)\n    second_process = re.sub(www_pat, '', first_process)\n    third_process = second_process.lower()\n    fourth_process = neg_pattern.sub(lambda x: negations_dic[x.group()], third_process)\n    result = re.sub(r'[^A-Za-z ]','',fourth_process)\n    return result.strip()"}, {"cell_type": "code", "execution_count": 118, "id": "798b2d6b-a1f9-41b8-a8b2-2f64f1dd6fce", "metadata": {}, "outputs": [], "source": "def save_predictions(predictions):\n    df_twtr = pd.read_csv(input_test)\n    #print(df_twtr.head(5))\n    # append predictions to dataframe\n    df_tweet_preds = df_twtr.copy()\n    #df_tweet_preds['predictions'] = predictions['prediction']\n    #print(df_tweet_preds.shape)\n    test = predictions.sample(False, .25, 42)\n    df_first_10 = test[['text', 'prediction']].head(10)\n    print(df_first_10)\n    \n    "}, {"cell_type": "code", "execution_count": 119, "id": "0c368c05-6d11-4ab1-838e-66e3c83a24e1", "metadata": {}, "outputs": [], "source": "# below main function can be use for either first training or getting predictions with a loaded model\n# first retrieve data\n# apply pre-processing by making the above defined pre_processing function to a user defined function\n# either build the pipeline from the above build_pipeline function and train or use a loaded pipeline model\n# make predictions on the test set\n# output the pipeline model, Spark dataframe of the predictions, and the prediction accuracy on the test set\n\ndef main(sqlc,input_dir,loaded_model):\n    print('retrieving data from {}'.format(input_dir))\n    model = loaded_model\n    print('preprocessing data...')\n    reg_replaceUdf = f.udf(pre_processing, t.StringType())\n    test_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(input_test)\n    test_set = test_set.withColumn('tweet', reg_replaceUdf(f.col('text')))\n    print('making predictions...')\n    predictions = model.transform(test_set)\n    # accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_set.count())\n    return model, predictions"}, {"cell_type": "code", "execution_count": 120, "id": "10abbf2f-7a6f-44a4-89d1-e42903e56527", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/tmp/ipykernel_2653/3063273455.py:13: UserWarning: SparkContext already exists in this scope\n  warnings.warn('SparkContext already exists in this scope')\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "retrieving data from gs://cloud-project-bucket-3\npreprocessing data...\nmaking predictions...\n"}, {"name": "stderr", "output_type": "stream", "text": "22/03/08 20:57:19 WARN org.apache.spark.ml.feature.StringIndexerModel: Input column sentiment does not exist during transformation. Skip StringIndexerModel for this column.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[Row(text='trent tells his fever dream about what the batman plot and the boys are also back for another week bone crushing recommendations for metal march drink the week pleasure chest ipa playalinda brewing company clown shoes undead party crasher heineken light', prediction=0.0), Row(text='you were terrific both movies you are real life ', prediction=0.0), Row(text='the batman 2022  ', prediction=0.0), Row(text='won\u2019 happen all can tell won\u2019 you can\u2019 have batman movies the same time batfleck finish done \u2019 not coming back after the flash movie', prediction=1.0), Row(text='\u2019 like see batman movie which someone who sees batman says \u201chey that guy dressed like batman like from the movies \u201d', prediction=0.0), Row(text='don\u2019 understand how people like ben affleck\u2019 batman ', prediction=0.0), Row(text='loved the fact that they emphasized the batman being detective pwede ulit kumain sinehan haha', prediction=0.0), Row(text='who the fuck said the batman was better than the dark knight \u2019all ain\u2019 true fans you said that movie was good ', prediction=0.0)]\npredictions finished!\n"}], "source": "if __name__==\"__main__\":\n    inputdir = \"gs://cloud-project-bucket-3\"\n    modeldir = \"gs://cloud-project-bucket-3/modeldir\"\n    outputfile = \"gs://cloud-project-bucket-3/outputfile.csv\"\n    \n    # create a SparkContext while checking if there is already SparkContext created\n    try:\n        sc = ps.SparkContext()\n        sc.setLogLevel(\"ERROR\")\n        sqlContext = ps.sql.SQLContext(sc)\n        print('Created a SparkContext')\n    except ValueError:\n        warnings.warn('SparkContext already exists in this scope')\n    # build pipeline, fit the model and retrieve the outputs by running main() function\n    loadedModel = PipelineModel.load(modeldir)\n    pipelineFit, predictions = main(sqlContext,inputdir,loadedModel)\n    save_predictions(predictions)\n    print('predictions finished!')\n    # print('accuracy on test data is {}'.format(accuracy))\n    # select the original target label 'sentiment', 'text' and 'label' created by label_stringIdx in the pipeline\n    # model predictions. Save it as a single CSV file to a destination specified by the second command line argument\n    # print('saving predictions to {}'.format(outputfile))\n    # predictions.select(predictions['sentiment'],predictions['text'],predictions['label'],predictions['prediction']).coalesce(1).write.mode(\"overwrite\").format(\"com.databricks.spark.csv\").option(\"header\", \"true\").csv(outputfile)\n    # save the trained model to destination specified by the third command line argument\n    # print('saving model to {}'.format(modeldir))\n    # pipelineFit.save(modeldir)\n    # Load the saved model and make another predictions on the same test set\n    # to check if the model was properly saved\n    #_, loaded_accuracy = main(sqlContext,inputdir,loadedModel)\n    # print('accuracy with saved model on test data is {}'.format(loaded_accuracy))\n    sc.stop()"}, {"cell_type": "code", "execution_count": 121, "id": "939d28b6-ddc5-4dfb-954a-3617e3fbd7d8", "metadata": {}, "outputs": [], "source": "sc.stop()"}, {"cell_type": "code", "execution_count": null, "id": "66d2f63d-fb9d-4234-8456-af85dbdd95f3", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}